{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zmc-8GQB0Drg"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, deque\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import gc\n",
        "from copy import deepcopy\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5dMoyy380Drk"
      },
      "outputs": [],
      "source": [
        "label_count_mapper = np.zeros((20, 20)) # since i know total number of docs I'm making this array directly\n",
        "class_docs = {} # how much docs each class has\n",
        "train_y = [] # training labels (1-20)\n",
        "conf_matrix = np.zeros((20, 20))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wHmVHgX1uQD",
        "outputId": "254826bb-9fca-4718-9728-d6d8c815078b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ5VWiIZ2CCj",
        "outputId": "9489ebb7-9a9d-4d45-91dd-d0154657c9be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hDqhcX30Drm",
        "outputId": "0e968fad-d9e8-4c69-b353-c952f65e13ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 480, 2: 581, 3: 572, 4: 587, 5: 575, 6: 592, 7: 582, 8: 592, 9: 596, 10: 594, 11: 598, 12: 594, 13: 591, 14: 594, 15: 593, 16: 599, 17: 545, 18: 564, 19: 464, 20: 376}\n"
          ]
        }
      ],
      "source": [
        "with open(f'./20news-bydate/matlab/train.label', 'r') as fp:\n",
        "    for line in fp:\n",
        "        label = int(line.strip())\n",
        "        # print(label)\n",
        "        # its good to point out that doc_id 'n' will have label at 'n-1'th place\n",
        "        # in simple words doc0 will have label at train_y[0] which will be between 1-20\n",
        "        train_y.append(label)\n",
        "        class_docs[label] = class_docs.get(label, 0) + 1\n",
        "print(class_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": false,
        "id": "_S-8fYtw0Drn"
      },
      "outputs": [],
      "source": [
        "previous_doc = 1\n",
        "freq = {} # (word, class): count\n",
        "with open(f'./20news-bydate/matlab/train.data', 'r') as fp:\n",
        "    word_ids = [] # this will basically contain the words and their counts\n",
        "    for line in fp:\n",
        "        doc_id, word_id, count = map(int, line.strip().split(' '))\n",
        "        # this means we have got all the words of our doc now we can take care of frequencies\n",
        "        # for doc1\n",
        "        # (1,5), (2,10), ...\n",
        "        # doc1 => class1\n",
        "        if previous_doc != doc_id:\n",
        "            # doc N will have label at (N-1)th place\n",
        "            class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
        "#             print(word_ids)\n",
        "            for word, word_count in word_ids:\n",
        "            # we are gonna have count of each word in each class in this dictionary\n",
        "            # example. hello, class1 => 20 times\n",
        "                freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
        "                previous_doc = doc_id\n",
        "                word_ids = [(word_id, count)]\n",
        "        else:\n",
        "            word_ids.append((word_id, count)) # appending word_id\n",
        "#         print(word_ids[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysKIAKgf0Dro",
        "outputId": "a1bab0ae-d5db-46d8-c546-f1680342cb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53975\n"
          ]
        }
      ],
      "source": [
        "# for the last doc\n",
        "class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
        "for word, count in word_ids:\n",
        "    freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
        "# del word_ids\n",
        "vocab = set([pair[0] for pair in freq.keys()]) #unique words\n",
        "# print(vocab)  # prints out the unique word ids\n",
        "v_len = len(vocab)\n",
        "print(v_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTa7UKx_0Drp",
        "outputId": "9e11b6f7-87a6-4bb3-aff0-ac2c565027d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 148812, 2: 110358, 3: 90767, 4: 99146, 5: 86190, 6: 152846, 7: 61094, 8: 114102, 9: 102631, 10: 107898, 11: 141267, 12: 200456, 13: 103173, 14: 155338, 15: 153714, 16: 201267, 17: 175914, 18: 254805, 19: 186426, 20: 118991}\n"
          ]
        }
      ],
      "source": [
        "# finding how many words does each class have\n",
        "class_words = {}\n",
        "for pair, word_count in freq.items():\n",
        "    class_ = pair[1]\n",
        "    class_words[class_] = class_words.get(class_, 0) + word_count\n",
        "print(class_words)\n",
        "total_docs = len(train_y) # this should be 11269 if using their indexing (see train.data)\n",
        "# sum(class_docs.values()) # again, this should be same as total_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8BfoHVVm0Drq"
      },
      "outputs": [],
      "source": [
        "prob_class = {}\n",
        "prob_word_class = {}\n",
        "\n",
        "# finding probability of each class\n",
        "for i in class_docs:\n",
        "    prob_class[i] = class_docs[i]/total_docs\n",
        "\n",
        "# findinf probability of each word in each class\n",
        "# we are doing this with smoothing too, for better results\n",
        "for word in vocab:\n",
        "    for class_ in class_words:\n",
        "        freq_class = freq.get((word, class_), 0)\n",
        "        # word/class\n",
        "        prob_word_class[(word, class_)] = (freq_class + 1)/(class_words[class_] + v_len)\n",
        "\n",
        "\n",
        "conf_matrix = np.zeros((20,20))\n",
        "previous_doc = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w0EpDH5S0Drq"
      },
      "outputs": [],
      "source": [
        "y_expected = []\n",
        "y_actual = []\n",
        "with open(f'./20news-bydate/matlab/test.label', 'r') as fp:\n",
        "  for line in fp:\n",
        "    y_expected.append(int(line.strip()))\n",
        "total_test_docs = len(y_expected)\n",
        "correct_classified = 0\n",
        "with open(f'./20news-bydate/matlab/test.data', 'r') as fp:\n",
        "  word_ids = [] # this will basically contain the words and their counts\n",
        "  j = 0\n",
        "  for line in fp:\n",
        "    doc_id, word_id, count = map(int, line.strip().split(' '))\n",
        "    if previous_doc != doc_id:\n",
        "      probs = deepcopy(prob_class)\n",
        "      for i in probs:\n",
        "        probs[i] = np.log(probs[i])\n",
        "      for word, word_count in word_ids:\n",
        "        for class_ in range(1,21):\n",
        "          # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
        "          probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
        "          # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
        "      _max_class = 1\n",
        "      _max_val = - np.inf\n",
        "      for i in probs:\n",
        "        if probs[i] > _max_val:\n",
        "          _max_val = probs[i]\n",
        "          _max_class = i\n",
        "      y_actual.append(_max_class)\n",
        "      # print(_max_class, end = ' ')\n",
        "      if y_expected[j] == _max_class:\n",
        "        correct_classified+=1\n",
        "      conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
        "      j += 1\n",
        "      previous_doc = doc_id\n",
        "      word_ids = [(word_id, count)]\n",
        "    else:\n",
        "      word_ids.append((word_id, count)) # appending word_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3CTg2hH0Drr",
        "outputId": "4cb55de3-2329-4e4f-8ea9-da7874d89dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7846768820786143\n"
          ]
        }
      ],
      "source": [
        "# this is for the last word_ids (code can be taken into function to remove redunduncy)\n",
        "for word, word_count in word_ids:\n",
        "  for class_ in range(1,21):\n",
        "    # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
        "    probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
        "    # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
        "_max_class = 1\n",
        "_max_val = - np.inf\n",
        "for i in probs:\n",
        "  if probs[i] > _max_val:\n",
        "    _max_val = probs[i]\n",
        "    _max_class = i\n",
        "# print(_max_class, end = ' ')\n",
        "y_actual.append(_max_class)\n",
        "if y_expected[j] == _max_class:\n",
        "  correct_classified+=1\n",
        "conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
        "\n",
        "incorrect_classified = total_test_docs - correct_classified\n",
        "\n",
        "print(correct_classified/total_test_docs)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Df5UW8a0Drs",
        "outputId": "7d070a34-ce81-4ef2-fa57-13450bf7d027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.7737449470140916\n",
            "(20, 20)\n",
            "[[240.   3.   3.   0.   1.   0.   0.   0.   0.   4.   2.   0.   2.  10.\n",
            "    4.   7.   1.  12.   7.  47.]\n",
            " [  0. 296.  33.   8.   8.  42.   9.   1.   1.   0.   0.   4.  17.   7.\n",
            "    8.   2.   0.   1.   1.   2.]\n",
            " [  0.   6. 209.  15.   9.   8.   4.   0.   0.   0.   0.   1.   0.   1.\n",
            "    0.   1.   0.   0.   0.   0.]\n",
            " [  0.  12.  60. 303.  36.  11.  46.   2.   0.   1.   0.   1.  28.   3.\n",
            "    0.   0.   0.   0.   0.   0.]\n",
            " [  0.   8.  10.  22. 277.   2.  21.   0.   0.   1.   0.   2.   7.   0.\n",
            "    0.   1.   1.   0.   0.   0.]\n",
            " [  1.  21.  30.   2.   2. 305.   0.   1.   0.   3.   0.   1.   3.   0.\n",
            "    1.   2.   0.   0.   1.   0.]\n",
            " [  0.   1.   0.   5.   5.   1. 235.   5.   1.   2.   0.   1.   1.   0.\n",
            "    0.   0.   1.   0.   0.   0.]\n",
            " [  0.   3.   1.   6.   4.   0.  31. 356.  25.   3.   1.   0.   9.   4.\n",
            "    0.   0.   3.   2.   1.   0.]\n",
            " [  0.   2.   2.   0.   1.   2.   5.   5. 353.   1.   0.   0.   2.   0.\n",
            "    1.   0.   1.   1.   0.   1.]\n",
            " [  0.   0.   2.   0.   1.   1.   0.   2.   2. 348.   4.   0.   0.   1.\n",
            "    0.   0.   1.   1.   0.   0.]\n",
            " [  1.   0.   1.   1.   0.   0.   1.   0.   0.  16. 383.   0.   1.   0.\n",
            "    1.   0.   1.   1.   0.   0.]\n",
            " [  1.  17.  16.   5.   4.  10.   3.   1.   1.   2.   0. 361.  46.   1.\n",
            "    4.   1.   3.   4.   3.   2.]\n",
            " [  1.   4.   1.  24.  16.   0.   9.   5.   1.   2.   0.   3. 260.   3.\n",
            "    4.   0.   0.   0.   0.   0.]\n",
            " [  2.   3.   4.   0.   8.   0.   2.   0.   1.   0.   2.   2.   6. 324.\n",
            "    4.   1.   1.   0.   3.   3.]\n",
            " [  3.   7.   4.   1.   2.   3.   3.   2.   0.   0.   1.   0.   4.   3.\n",
            "  336.   0.   2.   0.   7.   5.]\n",
            " [ 42.   4.   5.   0.   0.   1.   4.   1.   1.   3.   2.   2.   5.  17.\n",
            "    4. 377.   3.   7.   2.  68.]\n",
            " [  4.   0.   0.   0.   3.   1.   1.   5.   4.   1.   0.   8.   0.   3.\n",
            "    1.   3. 324.   3.  95.  19.]\n",
            " [  7.   1.   0.   0.   0.   1.   3.   1.   2.   2.   1.   0.   2.   7.\n",
            "    2.   1.   2. 325.   4.   5.]\n",
            " [  7.   1.   9.   0.   6.   2.   5.   8.   5.   8.   3.   8.   0.   9.\n",
            "   21.   1.  16.  19. 185.   7.]\n",
            " [  9.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
            "    1.   1.   4.   0.   1.  92.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print (\"F1 Score: \", sum(f1_score(y_expected, y_actual, average=None)/20))\n",
        "print(conf_matrix.shape)\n",
        "print (conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}